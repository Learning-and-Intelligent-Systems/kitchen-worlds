#!/usr/bin/env python

from __future__ import print_function
import shutil
import pickle
import os
import time
import random
import copy
import json
from os.path import join, abspath, dirname, isdir, isfile, basename
from config import EXP_PATH, OUTPUT_PATH

from pddlstream.language.constants import Equal, AND, print_solution, PDDLProblem
from pddlstream.algorithms.meta import solve, create_parser

from pybullet_tools.utils import disconnect, LockRenderer, has_gui, WorldSaver, wait_if_gui, \
    SEPARATOR, get_aabb, wait_for_duration, has_gui, reset_simulation, set_random_seed, \
    set_numpy_seed, set_renderer
from pybullet_tools.bullet_utils import summarize_facts, print_goal, nice, get_datetime
from pybullet_tools.pr2_agent import solve_multiple, post_process, pddlstream_from_state_goal, \
    create_cwd_saver, solve_one
from pybullet_tools.pr2_primitives import control_commands, apply_commands
from pybullet_tools.logging import parallel_print, myprint

from lisdf_tools.lisdf_loader import pddl_files_from_dir

from world_builder.world import State
from world_builder.actions import apply_actions
from world_builder.world_generator import save_to_outputs_folder

from test_utils import parallel_processing, get_config
from test_world_builder import create_pybullet_world

from nsplan_tools.generate_semantic_specification import get_semantic_specs, load_dict_from_json

# additional dependencies for using streams
from pybullet_tools.bullet_utils import set_camera_target_body, visualize_camera_image, get_readable_list
from pybullet_planning.pybullet_tools.utils import get_image_at_pose, get_image, unit_pose, get_camera_matrix
import matplotlib.pyplot as plt
from world_builder.entities import StaticCamera
from pybullet_tools.pr2_primitives import Pose, Conf
from pybullet_tools.utils import get_pose, multiply, quat_from_euler
from pybullet_tools.flying_gripper_utils import get_se3_joints, se3_from_pose
from world_builder.actions import get_primitive_actions

# gym-related
# import gymnasium as gym
# from gymnasium import error, spaces, utils
# from gymnasium.utils import seeding
import  gym
from gym import error, spaces, utils
from gym.utils import seeding
import numpy as np


DEFAULT_YAML = 'clean_dish_feg.yaml'
config = get_config(DEFAULT_YAML)


#####################################


def process_v0(input_dict):
    """ exist a version in cognitive-architectures for generating mini-datasets (single process),
        run in kitchen-worlds for parallelization, but no reliable planning time data

        inside each data folder, to be generated:
        - before planning:
            [x] scene.lisdf
            [x] problem.pddl
            [x] planning_config.json
            [x] log.txt (generated before planning)
        - after planning:
            [x] plan.json
            [x] commands.pkl
            [x] log.json (generated by pddlstream)
    """

    seed = input_dict["env_seed"]
    semantic_spec = input_dict["semantic_spec"]
    semantic_spec_seed = os.path.splitext(semantic_spec)[0]

    new_config = copy.deepcopy(config)
    new_config.seed = seed
    set_random_seed(seed)
    set_numpy_seed(seed)

    exp_dir = abspath(join(config.data.out_dir, "semantic_spec_{}_seed_{}_time_".format(semantic_spec_seed, seed))) #+ get_datetime(TO_LISDF=True)))
    print(exp_dir)
    os.makedirs(exp_dir, exist_ok=True)
    new_config.data.out_dir = exp_dir

    new_config.world.builder_kwargs["semantic_spec_file"] = os.path.join(config.semantic_specs_dir, semantic_spec)

    """ STEP 1 -- GENERATE SCENES """
    world, goal = create_pybullet_world(new_config, SAVE_LISDF=False, SAVE_TESTCASE=True)

    print(world)
    print(goal)

    saver = WorldSaver()

    """we want to prototype the gym environment below"""
    from pybullet_tools.bullet_utils import set_camera_target_body, visualize_camera_image
    from pybullet_planning.pybullet_tools.utils import get_image_at_pose, get_image, unit_pose, get_camera_matrix
    import matplotlib.pyplot as plt
    from world_builder.entities import StaticCamera

    # we need to be able to do a few things
    # https://www.etedal.net/2020/04/pybullet-panda_2.html
    # def __init__(self):
    #         ...
    #     def step(self, action):
    #         ...
    #     def reset(self):
    #         ...
    #     def render(self, mode='human'):
    #         ...
    #     def close(self):

    """let's try if we can render"""
    # 1. use world
    # 2. directly use bullet_utils
    # 3. world_builders/entities has StaticCamera class
    # 4. see test_camera

    # doesn't work
    # sink = world.name_to_body('sink#1')
    # camera_point, target_point = set_camera_target_body(sink, dx=3, dy=1, dz=1)
    #
    # camera_image = get_image(camera_point, target_point, width=640, height=480, vertical_fov=60.0, near=0.02, far=5.0,
    #                          tiny=True, segment=False)
    #
    # plt.imshow(camera_image.rgbPixels)
    # plt.show()

    # doesn't work
    # camera_pose = unit_pose()
    # sink = world.name_to_body('sink#1')
    # camera_point, target_point = set_camera_target_body(sink, dx=3, dy=1, dz=1)
    # camera_kwarg = {'camera_point': camera_point, 'target_point': target_point}
    # width = 1280
    # height = 960
    # fx = 800
    # crop = False
    # N_PX = 224
    # viz_dir ="/home/weiyu/Research/nsplan/original/kitchen-worlds/outputs"
    #
    # common = dict(img_dir=viz_dir, width=width // 2, height=height // 2, fx=fx // 2)
    # crop_kwargs = dict(crop=crop, center=crop, width=width // 2, height=height // 2, N_PX=N_PX)
    #
    # # world.add_camera(camera_pose, **common, **camera_kwarg)
    #
    # camera_image = world.camera.get_image(segment=True, segment_links=True)
    #
    # plt.imshow(camera_image.rgbPixels)
    # plt.show()

    # this works
    width = 1280
    height = 960
    fx = 800
    camera_matrix = get_camera_matrix(width=width, height=height, fx=fx)
    camera = StaticCamera(unit_pose(), camera_matrix=camera_matrix)
    sink = world.name_to_body('sink#1')
    camera_point, target_point = set_camera_target_body(sink, dx=3, dy=0, dz=1)
    # we can set tiny to true when not using gui
    # https://github.com/bulletphysics/bullet3/issues/1157
    camera_image = camera.get_image(camera_point=camera_point, target_point=target_point, tiny=True)
    # plt.imshow(camera_image.rgbPixels)
    # plt.show()



    """let's try how we can step"""
    # should return observation, reward, done, info

    from pybullet_tools.pr2_primitives import Pose, Conf
    from pybullet_tools.utils import get_pose, multiply, quat_from_euler
    from pybullet_tools.flying_gripper_utils import get_se3_joints, se3_from_pose
    from world_builder.actions import get_primitive_actions


    ## need to enumerate actions that we can take
    # which object to manipulate
    # which action to take: grasp, move, place
    # see quick_demo() in pybullet_planning/pybullet_tools/flying_gripper_utils.py for examples of how to use feg streams



    ## need to compute symbolic state and check if we are at goal
    state = State(world)
    robot = state.robot
    world = state.world
    problem = state

    domain_path = abspath(config.planner.domain_pddl)
    stream_map = robot.get_stream_map(problem, collisions=not config.cfree, custom_limits=world.robot.custom_limits,
                                      teleport=config.teleport, domain_pddl=domain_path, num_grasp_samples=30)

    # dict_keys(['sample-pose-on', 'sample-pose-in', 'sample-grasp', 'inverse-kinematics-hand', 'test-cfree-pose-pose',
    # 'test-cfree-approach-pose', 'test-cfree-traj-pose', 'plan-free-motion-hand', 'get-joint-position-open',
    # 'sample-handle-grasp', 'inverse-kinematics-grasp-handle', 'plan-grasp-pull-handle', 'get-pose-from-attachment',
    # 'test-plate-in-cabinet', 'test-seconf-close-to-surface'])

    # let's try sample-grasp

    mug_1_body = world.name_to_body('bowl#1')
    mug_1_object =world.name_to_object('bowl#1')

    mug_1_pose = mug_1_object.get_link_pose(link=-1)
    print(mug_1_pose)
    print(get_pose(mug_1_body))

    mug_1_Pose = Pose(mug_1_body, get_pose(mug_1_body))
    print(mug_1_Pose)

    current_q = Conf(robot, get_se3_joints(robot))


    # grasps_set = set()
    # for i in range(10):
    #     grasp_list = next(stream_map["sample-grasp"](body=mug_1_body))
    #     for grasp in grasp_list:
    #         if grasp not in grasps_set:
    #             print(grasp[0])
    #             grasps_set.add(grasp[0].value)
    #         else:
    #             print("repeat in {}".format(i))
    #     print(i, grasp_list)
    #     print(len(grasps_set))
    # exit()

    # return a list of tuple [(grasp, )]
    for grasp_list in stream_map["sample-grasp"](body=mug_1_body):
        # grasp_list = next(stream_map["sample-grasp"](body=mug_1_body), None)
        print(grasp_list)

    for grasp_list in stream_map["sample-grasp"](body=mug_1_body):
        # grasp_list = next(stream_map["sample-grasp"](body=mug_1_body), None)
        print(grasp_list)

    for grasp_list in stream_map["sample-grasp"](body=mug_1_body):
        # grasp_list = next(stream_map["sample-grasp"](body=mug_1_body), None)
        print(grasp_list)

        input("grasp sampled")

        for grasp in grasp_list:
            print(grasp)
            for ik in stream_map["inverse-kinematics-hand"](a=None, o=mug_1_body, p=mug_1_Pose, g=grasp[0]):
                if len(ik) == 0:
                    continue

                # TODO: some approach pose for grasping is very strange, may need to use test-cfree-approach-pose

                ## this block executes the pick
                # print(ik)
                # input("plan motion for this ik")
                #
                # action = ('pick_hand', ('hand', mug_1_body, mug_1_Pose, grasp[0], None, ik[0][1]))
                # commands = get_primitive_actions(action, world)
                # print(commands)
                #
                # saver.restore()
                # input("apply actions")
                # set_renderer(True)
                # apply_actions(state, commands, time_step=config.time_step, verbose=False)

                ## sanity check, these two joint values should be the same
                # print("check for approach q")
                # print("after executing grasp", Conf(robot, get_se3_joints(robot)))
                # print("from grasp", ik[0][0])

                ## sanity check
                # TODO: why q2 and q2_debug have different values?
                # in ik function, the following line exists
                # body_pose = robot.get_body_pose(o, verbose=False)
                # approach_pose = multiply(body_pose, g.approach)
                # g = grasp[0]
                # body_pose = robot.get_body_pose(mug_1_body, verbose=False)
                # approach_pose = multiply(body_pose, g.approach)
                # seconf2 = se3_from_pose(approach_pose)
                # q2 = ik[0][0]
                # q2_debug = Conf(robot, get_se3_joints(robot), seconf2)
                # print("check for approach q")
                # print("from grasp", q2)
                # print("from grasp", q2_debug)
                #
                # print(q2.values)
                # print(q2_debug.values)
                # print(quat_from_euler(list(q2.values)[3:]))
                # print(quat_from_euler(list(q2_debug.values)[3:]))

                ## now we want to sample a trajectory from current position to next
                q2 = ik[0][0]
                for raw_path in stream_map["plan-free-motion-hand"](q1=current_q, q2=q2):
                    print(raw_path)
                    if len(raw_path) == 0:
                        continue

                    # ## this block executes the move and the pick
                    # pick_action = ('pick_hand', ('hand', mug_1_body, mug_1_Pose, grasp[0], None, ik[0][1]))
                    # move_action = ('move_cartesian', (current_q, ik[0][0], raw_path[0][0]))
                    #
                    # commands = []
                    # for action in [move_action, pick_action]:
                    #     commands += get_primitive_actions(action, world)
                    #
                    # saver.restore()
                    # # see facts before executing the actions
                    # get_facts(world, state)
                    # input("apply actions")
                    # set_renderer(True)
                    # apply_actions(state, commands, time_step=config.time_step, verbose=False)
                    # # see facts before executing the actions
                    # get_facts(world, state)

                    ## now the object is in hand, we want to see how to place the object

                    # LOCATIONS = ["sink_counter_left", "sink_counter_right", "shelf_lower", "sink_bottom", "cabinettop_storage"]
                    surface = world.name_to_body('shelf_lower')
                    for placement_pose in stream_map["sample-pose-on"](body=mug_1_body, surface=surface):
                        print(placement_pose)

                        for ik2 in stream_map["inverse-kinematics-hand"](a=None, o=mug_1_body, p=placement_pose[0][0], g=grasp[0]):
                            print(ik2)
                            if len(ik2) == 0:
                                continue

                            ## motion plan to place
                            q3 = ik2[0][0]
                            for raw_path2 in stream_map["plan-free-motion-hand"](q1=q2, q2=q3):
                                print(raw_path2)
                                if len(raw_path2) == 0:
                                    continue

                                ## this block executes the move and the pick
                                move_action = ('move_cartesian', (current_q, q2, raw_path[0][0]))
                                pick_action = ('pick_hand', ('hand', mug_1_body, mug_1_Pose, grasp[0], None, ik[0][1]))
                                move_action_2 = ('move_cartesian', (q2, q3, raw_path2[0][0]))
                                place_action = ('place_hand', ('hand', mug_1_body, placement_pose, grasp[0], None, ik2[0][1]))

                                commands = []
                                for action in [move_action, pick_action, move_action_2, place_action]:
                                    commands += get_primitive_actions(action, world)

                                saver.restore()
                                # see facts before executing the actions
                                get_facts(world, state)
                                input("apply actions")
                                set_renderer(True)
                                apply_actions(state, commands, time_step=config.time_step, verbose=False)
                                # see facts before executing the actions
                                get_facts(world, state)




                        input("next placement pose?")










            input("ik for next grasp?")







    # saver = WorldSaver()
    #
    # domain_path = abspath(config.planner.domain_pddl)
    # stream_path = abspath(config.planner.stream_pddl)
    #
    # print_fn = parallel_print ## if args.parallel else myprint
    # print_fn(config)
    #
    # state = State(world)
    # pddlstream_problem = pddlstream_from_state_goal(
    #     state, goal, domain_pddl=domain_path, stream_pddl=stream_path,
    #     custom_limits=world.robot.custom_limits, collisions=not config.cfree,
    #     teleport=config.teleport, print_fn=print_fn)
    # stream_info = world.robot.get_stream_info()
    #
    # kwargs = {'visualize': False}
    # if config.planner.diverse:
    #     kwargs.update(dict(
    #         diverse=True,
    #         downward_time=config.planner.downward_time,  ## max time to get 100, 10 sec, 30 sec for 300
    #         evaluation_time=60,  ## on each skeleton
    #         max_plans=200,  ## number of skeletons
    #     ))
    # start = time.time()
    #
    # # solution, tmp_dir = solve_one(pddlstream_problem, stream_info, lock=config.lock, **kwargs)
    #
    # cwd_saver = create_cwd_saver()
    # solution, tmp_dir = solve_multiple(pddlstream_problem, stream_info, lock=config.lock,
    #                                    cwd_saver=cwd_saver, **kwargs)
    #
    # print_solution(solution)
    # plan, cost, evaluations = solution
    #
    # """ =============== log plan and planning time =============== """
    # t = None if config.parallel else round(time.time() - start, 3)
    # if plan is None:
    #     plan_log = None
    #     plan_len = None
    #     init = None
    # else:
    #     plan_log = [str(a) for a in plan]
    #     plan_len = len(plan)
    #     init = [[str(a) for a in f] for f in evaluations.preimage_facts]
    # time_log = [{
    #     'planning': t, 'plan': plan_log, 'plan_len': plan_len, 'init': init
    # }, {'total_planning': t}]
    # with open(join(exp_dir, f'plan.json'), 'w') as f:
    #     json.dump(time_log, f, indent=4)
    #
    # """ =============== save planing log =============== """
    # txt_file = join(tmp_dir, 'txt_file.txt')
    # if isfile(txt_file):
    #     shutil.move(txt_file, join(exp_dir, f"log.txt"))
    # viz_dir = join(tmp_dir, 'visualizations')
    # if isdir(viz_dir):
    #     shutil.move(viz_dir, join(exp_dir, 'visualizations'))
    # cwd_saver.restore()
    #
    # """ =============== save commands for replay =============== """
    # with LockRenderer(lock=config.lock):
    #     commands = post_process(state, plan)
    #     state.remove_gripper()
    #     saver.restore()
    # with open(join(exp_dir, f"commands.pkl"), 'wb') as f:
    #     pickle.dump(commands, f)
    #
    # """ =============== visualize the plan =============== """
    # if (plan is None) or not has_gui():
    #     reset_simulation()
    #     disconnect()
    #     return
    #
    # print(SEPARATOR)
    # saver.restore()
    # # wait_if_gui('Execute?')
    # if config.simulate:  ## real physics
    #     control_commands(commands)
    # else:
    #     set_renderer(True)
    #     apply_actions(state, commands, time_step=config.time_step, verbose=False)

    wait_if_gui('Finish?')
    print(SEPARATOR)
    reset_simulation()
    disconnect()

def get_env():
    """ 
    Geng Chen: get environment
    """
    if config.env_seed is not None and config.semantic_spec is not None:
        inputs = [{"semantic_spec": config.semantic_spec, "env_seed": config.env_seed}]
    else:
        semantic_specs = get_semantic_specs(config.semantic_specs_dir)
        inputs = []
        for spec in semantic_specs[23:]:
            for env_seed in range(10):
                inputs.append({"semantic_spec": spec, "env_seed": env_seed})
    print(config)
    input_dict = inputs[0]
    seed = input_dict["env_seed"]
    semantic_spec = input_dict["semantic_spec"]
    semantic_spec_seed = os.path.splitext(semantic_spec)[0]

    

    new_config = copy.deepcopy(config)
    new_config.seed = seed
    set_random_seed(seed)
    set_numpy_seed(seed)

    semantic_spec_file = os.path.join(config.semantic_specs_dir, semantic_spec)
    semantic_spec_dict = load_dict_from_json(semantic_spec_file)
    obj_dict = semantic_spec_dict["objects"]
    goal_dict =semantic_spec_dict["goals"]
    # print('--------------')
    # print(obj_dict)
    # print(goal_dict)
    new_config.obj_dict = obj_dict
    new_config.goal_dict = goal_dict

    exp_dir = abspath(join(config.data.out_dir, "semantic_spec_{}_seed_{}_time_".format(semantic_spec_seed, seed))) #+ get_datetime(TO_LISDF=True)))
    os.makedirs(exp_dir, exist_ok=True)
    new_config.data.out_dir = exp_dir

    new_config.world.builder_kwargs["semantic_spec_file"] = os.path.join(config.semantic_specs_dir, semantic_spec)

    world, goal = create_pybullet_world(new_config, SAVE_LISDF=False, SAVE_TESTCASE=True)

    env = CleanDishEnvV1(world, goal, new_config, render_mode="human")
    env.reset()
    return env

def process(input_dict):
    """ exist a version in cognitive-architectures for generating mini-datasets (single process),
        run in kitchen-worlds for parallelization, but no reliable planning time data

        inside each data folder, to be generated:
        - before planning:
            [x] scene.lisdf
            [x] problem.pddl
            [x] planning_config.json
            [x] log.txt (generated before planning)
        - after planning:
            [x] plan.json
            [x] commands.pkl
            [x] log.json (generated by pddlstream)
    """

    seed = input_dict["env_seed"]
    semantic_spec = input_dict["semantic_spec"]
    semantic_spec_seed = os.path.splitext(semantic_spec)[0]

    new_config = copy.deepcopy(config)
    new_config.seed = seed
    set_random_seed(seed)
    set_numpy_seed(seed)

    exp_dir = abspath(join(config.data.out_dir, "semantic_spec_{}_seed_{}_time_".format(semantic_spec_seed, seed))) #+ get_datetime(TO_LISDF=True)))
    print(exp_dir)
    os.makedirs(exp_dir, exist_ok=True)
    new_config.data.out_dir = exp_dir

    new_config.world.builder_kwargs["semantic_spec_file"] = os.path.join(config.semantic_specs_dir, semantic_spec)

    """ STEP 1 -- GENERATE SCENES """

    world, goal = create_pybullet_world(new_config, SAVE_LISDF=False, SAVE_TESTCASE=True)

    env = CleanDishEnvV1(world, goal, config, render_mode="human")
    #env = CleanDishEnvV1(world, goal, new_config, render_mode="human")
    env.reset()

    input("env initialized, next?")

    play(env)

from PIL import Image
def play(env):
    try:
        done = False
        obs = env.reset()
        #print(obs[0])
        # print(obs[0][0])
        image = Image.fromarray(obs[0][0])
        image.save('kitchen.png')
        num_moves = 0
        while not done:
            print("-"*10)
            print(f"move {num_moves}")
            env.print_admissible_actions()
            manip_name = input("manipulation name > ")
            obj_name = input("object name > ")
            loc_name = input("location name > ")
            action = env.convert_text_to_action(manip_name, obj_name, loc_name)
            obs, score, done, _, info = env.step(action)
            image = Image.fromarray(obs[0])
            image.save('kitchen.png')
            print(f"\nscore {score}, done {done}")
            num_moves += 1
    except KeyboardInterrupt:
        pass # Press the stop button in the toolbar to quit the game.

    print("\n" + "*" * 100)
    print("Played {} steps, scoring {} points.".format(num_moves, score))
    print("*" * 100 + "\n")


def debug_play(env):
    try:
        done = False
        obs = env.reset()
        num_moves = 0

        for mi in range(4):
            print("-"*10)
            print(f"move {num_moves}")
            input("execute move?")
            if mi == 0:
                action = env.convert_text_to_action("pick", "bowl#1", "sink_counter_left")
            if mi == 1:
                action = env.convert_text_to_action("place", "bowl#1", "cabinettop_storage")
            if mi == 2:
                action = env.convert_text_to_action("pick", "bowl#1", "shelf_lower")
            if mi == 3:
                action = env.convert_text_to_action("place", "bowl#1", "cabinettop_storage")
            obs, score, done, _, info = env.step(action)
            print(f"score {score}, done {done}")
            num_moves += 1
    except KeyboardInterrupt:
        pass # Press the stop button in the toolbar to quit the game.

    print("\n" + "*"*100)
    print("Played {} steps, scoring {} points.".format(num_moves, score))
    print("*" * 100 + "\n")

class CleanDishEnvV0(gym.Env):

    """
    This environment is for mainly for debug purpose. It only supports one object and one location
    """

    # TODO: do we need to differentiate observation and render?
    #       observation may be low-level object and robot state

    metadata = {'render_modes': ['human', 'rgb_array']}

    def __init__(self, world, goal, config, render_mode):
        self.world = world
        self.goal = goal
        self.config = config

        self.saver = WorldSaver()

        state = State(world)
        self.state = state
        self.robot = state.robot
        self.problem = state

        domain_path = abspath(self.config.planner.domain_pddl)
        self.stream_map = self.robot.get_stream_map(self.problem, collisions=not self.config.cfree,
                                                    custom_limits=self.world.robot.custom_limits,
                                                    teleport=self.config.teleport, domain_pddl=domain_path,
                                                    num_grasp_samples=30)

        # debug env, only manipulate one object
        self.debug_object_name = 'bowl#1'
        self.debug_surface_name = 'shelf_lower'

        # Here's an observation space for 200 wide x 100 high RGB image inputs:
        self.observation_space = spaces.Box(low=0, high=255, shape=(1280, 960, 3), dtype=np.uint8)
        # use a list of cameras: https://stackoverflow.com/questions/68113725/openai-gym-observation-space-representation

        # use text action: https://github.com/microsoft/TextWorld/blob/main/notebooks/Playing%20TextWorld%20generated%20games%20with%20OpenAI%20Gym.ipynb
        # pick obj, place obj
        self.action_space = spaces.Discrete(2)

        assert render_mode is None or render_mode in self.metadata["render_modes"]
        self.render_mode = render_mode

        # record some state information
        # TODO: maybe this information is redundant and we can get it from elsewhere, like world, saver, ..
        self.current_g = None

    def step(self, action):
        # return observation, reward, terminated, False, info

        ## compute action
        commands = None

        # TODO: it seems like these stream functions will also change the world state, we need to be able to save state
        #       before planning and revert after planning
        if action == 0:
            # execute pick
            commands = self._get_pick_action(self.debug_object_name)
        elif action == 1:
            # execute place
            commands = self._get_place_action(self.debug_object_name, self.debug_surface_name)

        ## step env and compute reward
        if commands is None:
            terminated = False
            reward = -100
        else:
            set_renderer(True)
            apply_actions(self.state, commands, time_step=config.time_step, verbose=False)

            terminated = False
            reward = 1 if terminated else 0  # Binary sparse rewards

        ## update observation
        observation = self._get_obs()
        info = self._get_info()

        return observation, reward, terminated, False, info

    def reset(self):
        self.saver.restore()

        observation = self._get_obs()
        info = self._get_info()
        return observation


    def render(self, mode='human'):
        camera_image = self._get_obs()

        if mode == 'human':
            plt.imshow(camera_image.rgbPixels)
            plt.show()

        return camera_image

    def close(self):
        reset_simulation()
        disconnect()

    def _get_obs(self):
        # return {"agent": self._agent_location, "target": self._target_location}

        width = 1280
        height = 960
        fx = 800
        camera_matrix = get_camera_matrix(width=width, height=height, fx=fx)
        camera = StaticCamera(unit_pose(), camera_matrix=camera_matrix)
        sink = self.world.name_to_body('sink#1')
        camera_point, target_point = set_camera_target_body(sink, dx=3, dy=0, dz=1)
        # we can set tiny to true when not using gui
        # https://github.com/bulletphysics/bullet3/issues/1157
        camera_image = camera.get_image(camera_point=camera_point, target_point=target_point, tiny=False)
        # plt.imshow(camera_image.rgbPixels)
        # plt.show()
        return camera_image

    def _get_info(self):
        # return {
        #     "distance": np.linalg.norm(
        #         self._agent_location - self._target_location, ord=1
        #     )
        # }

        return {}

    def _get_pick_action(self, object_name):

        ## before we do anything
        if self.current_g is not None:
            # there is already object in hand
            return None

        # get current robot configuration
        current_q = Conf(self.robot, get_se3_joints(self.robot))

        obj_body = self.world.name_to_body(object_name)
        world_obj = self.world.name_to_object(object_name)

        # get current pose and instantiate Pose()
        obj_pose = world_obj.get_link_pose(link=-1)
        obj_Pose = Pose(obj_body, get_pose(obj_body))

        # return a list of tuple [(grasp, )]
        grasp_list = next(self.stream_map["sample-grasp"](body=obj_body))
        print("sample {} grasps for {}: {}".format(len(grasp_list), world_obj, grasp_list))

        for grasp in grasp_list:
            print("find ik for grasp", grasp)

            ## find ik
            for ik in self.stream_map["inverse-kinematics-hand"](a=None, o=obj_body, p=obj_Pose, g=grasp[0]):
                if len(ik) == 0:
                    continue

                ## now we want to sample a trajectory from current position to next
                q2 = ik[0][0]
                for move_cmd in self.stream_map["plan-free-motion-hand"](q1=current_q, q2=q2):
                    print(move_cmd)
                    if len(move_cmd) == 0:
                        continue

                    ## computes actions to step the world
                    pick_action = ('pick_hand', ('hand', obj_body, obj_Pose, grasp[0], None, ik[0][1]))
                    move_action = ('move_cartesian', (current_q, ik[0][0], move_cmd[0][0]))
                    commands = []
                    for action in [move_action, pick_action]:
                        commands += get_primitive_actions(action, self.world)

                    ## update world state
                    self.current_g = grasp[0]

                    return commands

        return None

    def _get_place_action(self, object_name, surface_name):

        ## before we do anything
        if self.current_g is None:
            # there is nothing in hand to be placed
            return None

        # get current robot configuration
        current_q = Conf(self.robot, get_se3_joints(self.robot))

        surface = self.world.name_to_body(surface_name)

        obj_body = self.world.name_to_body(object_name)

        for placement_pose in self.stream_map["sample-pose-on"](body=obj_body, surface=surface):

            print("find ik for placement pose", placement_pose)

            for ik in self.stream_map["inverse-kinematics-hand"](a=None, o=obj_body, p=placement_pose[0][0], g=self.current_g):
                print(ik)
                if len(ik) == 0:
                    continue

                ## motion plan to place
                q = ik[0][0]
                for move_cmd in self.stream_map["plan-free-motion-hand"](q1=current_q, q2=q):
                    print(move_cmd)
                    if len(move_cmd) == 0:
                        continue

                    ## computes actions to step the world
                    move_action = ('move_cartesian', (current_q, q, move_cmd[0][0]))
                    place_action = ('place_hand', ('hand', obj_body, placement_pose, self.current_g, None, ik[0][1]))
                    commands = []
                    for action in [move_action, place_action]:
                        commands += get_primitive_actions(action, self.world)

                    ## update world state
                    self.current_g = None

                    return commands

        return None


class CleanDishEnvV1(gym.Env):
    """
    This environment supports multiple objects and locations
    """

    # TODO: do we need to differentiate observation and render?
    #       observation may be low-level object and robot state

    metadata = {'render_modes': ['human', 'rgb_array']}

    def __init__(self, world, goal, config, render_mode):
        print("Initializing env ...")
        self.world = world
        self.goal = goal
        self.config = config

        self.saver = WorldSaver()

        state = State(world)
        self.state = state
        self.robot = state.robot
        self.problem = state

        domain_path = abspath(self.config.planner.domain_pddl)
        self.stream_map = self.robot.get_stream_map(self.problem, collisions=not self.config.cfree,
                                                    custom_limits=self.world.robot.custom_limits,
                                                    teleport=self.config.teleport, domain_pddl=domain_path,
                                                    num_grasp_samples=30)

        ## get movable objects
        moveable_bodies = world.cat_to_bodies('moveable')
        object_names = [world.body_to_name(b) for b in moveable_bodies]
        print("moveable objects", object_names)

        ## get locations
        locations = ["sink_counter_left", "sink_counter_right", "shelf_lower", "sink_bottom", "cabinettop_storage"]
        print("locations", locations)

        # use text action: https://github.com/microsoft/TextWorld/blob/main/notebooks/Playing%20TextWorld%20generated%20games%20with%20OpenAI%20Gym.ipynb
        # pick obj, place obj
        self.manipulation_name_to_id = {}
        self.object_name_to_id = {}
        self.location_name_to_id = {}
        self.set_up_text_actions(object_names, locations)
        self.id_to_manipulation_name = self.reverse_vocab(self.manipulation_name_to_id)
        self.id_to_object_name = self.reverse_vocab(self.object_name_to_id)
        self.id_to_location_name = self.reverse_vocab(self.location_name_to_id)

        self.action_space = spaces.MultiDiscrete([len(self.manipulation_name_to_id),
                                                  len(self.object_name_to_id),
                                                  len(self.location_name_to_id)])

        # Here's an observation space for 200 wide x 100 high RGB image inputs:
        self.observation_space = spaces.Box(low=0, high=255, shape=(1280, 960, 3), dtype=np.uint8)
        # use a list of cameras: https://stackoverflow.com/questions/68113725/openai-gym-observation-space-representation

        assert render_mode is None or render_mode in self.metadata["render_modes"]
        self.render_mode = render_mode

        # record some state information
        # TODO: maybe this information is redundant and we can get it from elsewhere, like world, saver, ..
        self.current_g = None
        self.commands_so_far = []
        # map from each object to its information
        self.symbolic_state = {}
        self.initialize_symbolic_state()
        self.update_symbolic_state()

        print(f"\ngoal {goal}\n")
        self.symbolic_goal = {}
        self.parse_symbolic_goal()

        #CG:object descriptions and mission in language
        self.object_name_to_des={}
        print(self.object_name_to_id)
        goal_obj = list(self.symbolic_goal.keys())[0]
        goal_loc = list(self.symbolic_goal.values())[0]
        for key, value in self.config.obj_dict.items():
            print(value.get('class'))
            print(value.get('location'))
            if value.get('class') == goal_obj.split('#')[0]:
                goal_obj_color = value.get('color')
        self.mission = 'place ' + goal_obj_color + ' '+ goal_obj.split('#')[0] + ' ' + goal_loc

    def print_admissible_actions(self):
        print("Manipulation actions:", sorted(list(self.manipulation_name_to_id.keys())))
        print("Objects:", sorted(list(self.object_name_to_id.keys())))
        print("Locations:", sorted(list(self.location_name_to_id.keys())))

    def set_up_text_actions(self, object_names, locations):
        #self.manipulation_name_to_id = {"pick": 0, "place": 1}
        self.manipulation_name_to_id = {"pick": 0, "place": 1, "clean": 2}

        self.object_name_to_id = {}
        for object_name in sorted(object_names):
            self.object_name_to_id[object_name] = len(self.object_name_to_id)

        self.location_name_to_id = {}
        for location_name in sorted(locations):
            self.location_name_to_id[location_name] = len(self.location_name_to_id)

    def reverse_vocab(self, vocab):
        return {vocab[v]: v for v in vocab}

    def convert_text_to_action(self, manipulation, object, location):
        return [self.manipulation_name_to_id[manipulation],
                self.object_name_to_id[object],
                self.location_name_to_id[location]]

    def convert_action_to_text(self, action):
        return self.id_to_manipulation_name[action[0]], self.id_to_object_name[action[1]], self.id_to_location_name[action[2]]

    def step(self, action):
        # return observation, reward, terminated, False, info

        ## compute env commands
        commands = None

        # TODO: it seems like these stream functions will also change the world state, we need to be able to save state
        #       before planning and revert after planning
        manip_name, obj_name, loc_name = self.convert_action_to_text(action)

        symbolically_feasible = self.check_symbolic_action_feasibility(manip_name, obj_name, loc_name)

        if symbolically_feasible:
            # saver = WorldSaver()
            with LockRenderer(lock=True):
                if manip_name == "pick":
                    # execute pick
                    commands = self._get_pick_action(obj_name)
                elif manip_name == "place":
                    # execute place
                    commands = self._get_place_action(obj_name, loc_name)
            # TODO: we want to be able to restore to the state before planning, but right not grasp attachements are not
            #       correctly restored.
            # saver.restore()
            # self.state.assign()

        ## step env and compute reward
        # if symbolically_feasible is False or commands is None:
        if symbolically_feasible is False:
            terminated = False
            reward = -1
        else:
            # set_renderer(True)
            # apply_actions(self.state, commands, time_step=config.time_step, verbose=False)

            # debug: right now we are restoring the world to the initial state and execute all actions planned so far.
            #        once we have way to restore to any particular time of the execution, we don't need to do this anymore.
            if manip_name != 'clean':
                self.commands_so_far += commands
            self.state.remove_gripper()
            self.saver.restore()
            set_renderer(True)
            apply_actions(self.state, self.commands_so_far, time_step=0, verbose=False)

            # update symbolic state
            #self.update_symbolic_state(target_obj_name=obj_name)
            self.update_symbolic_state(target_obj_name=obj_name, target_manip_name = manip_name)

            terminated = self.check_reach_symbolic_goal()
            reward = 100 if terminated else 0  # Binary sparse rewards

        ## update observation
        observation = self._get_obs()
        info = self._get_info()

        return observation, reward, terminated, False, info

    def reset(self):
        self.saver.restore()

        observation = self._get_obs()
        info = self._get_info()
        return observation, info

    def render(self, mode='human'):
        camera_image = self._get_obs()

        if mode == 'human':
            plt.imshow(camera_image.rgbPixels)
            plt.show()

        return camera_image

    def close(self):
        reset_simulation()
        disconnect()

    def _get_obs(self):
        # return {"agent": self._agent_location, "target": self._target_location}

        width = 1280
        height = 960
        fx = 800
        camera_matrix = get_camera_matrix(width=width, height=height, fx=fx)
        camera = StaticCamera(unit_pose(), camera_matrix=camera_matrix)
        sink = self.world.name_to_body('sink#1')
        camera_point, target_point = set_camera_target_body(sink, dx=2, dy=0, dz=1)
        # we can set tiny to true when not using gui
        # https://github.com/bulletphysics/bullet3/issues/1157

        # set tiny to True when we are not using GUI
        tiny = not self.config.viewer
        camera_image = camera.get_image(camera_point=camera_point, target_point=target_point, tiny=tiny)

        if self.render_mode == "human":
            plt.imshow(camera_image.rgbPixels)
            plt.show()
        return camera_image

    def _get_obs_multiview(self):
        # return {"agent": self._agent_location, "target": self._target_location}

        width = 1280
        height = 960
        fx = 800
        camera_matrix = get_camera_matrix(width=width, height=height, fx=fx)
        camera = StaticCamera(unit_pose(), camera_matrix=camera_matrix)
        sink = self.world.name_to_body('sink#1')
        camera_point1, target_point1 = set_camera_target_body(sink, dx=2, dy=0, dz=1)
        sink = self.world.name_to_body('sink#1')
        camera_point2, target_point2 = set_camera_target_body(sink, dx=0.01, dy=0, dz=1)
        cabinettop = self.world.name_to_body('cabinettop')
        camera_point3, target_point3 = set_camera_target_body(cabinettop, dx=0.01, dy=0, dz=1)
        shelf_lower = self.world.name_to_body('shelf_lower')
        camera_point4, target_point4 = set_camera_target_body(shelf_lower, dx=0.01, dy=0, dz=1)
        # we can set tiny to true when not using gui
        # https://github.com/bulletphysics/bullet3/issues/1157

        # set tiny to True when we are not using GUI
        tiny = not self.config.viewer
        camera_image1 = camera.get_image(camera_point=camera_point1, target_point=target_point1, tiny=tiny)
        camera_image2 = camera.get_image(camera_point=camera_point2, target_point=target_point2, tiny=tiny)
        camera_image3 = camera.get_image(camera_point=camera_point3, target_point=target_point3, tiny=tiny)
        camera_image4 = camera.get_image(camera_point=camera_point4, target_point=target_point4, tiny=tiny)

        camera_images = [camera_image1, camera_image2, camera_image3, camera_image4]
        # if self.render_mode == "human":
        #     plt.imshow(camera_image.rgbPixels)
        #     plt.show()
        return camera_images

    def _get_info(self):
        # return {
        #     "distance": np.linalg.norm(
        #         self._agent_location - self._target_location, ord=1
        #     )
        # }

        return {}

    def _get_pick_action(self, object_name):

        ## before we do anything
        if self.current_g is not None:
            # there is already object in hand
            return None

        # get current robot configuration
        current_q = Conf(self.robot, get_se3_joints(self.robot))

        obj_body = self.world.name_to_body(object_name)
        world_obj = self.world.name_to_object(object_name)

        # get current pose and instantiate Pose()
        obj_pose = world_obj.get_link_pose(link=-1)
        obj_Pose = Pose(obj_body, get_pose(obj_body))

        # return a list of tuple [(grasp, )]
        grasp_list = next(self.stream_map["sample-grasp"](body=obj_body))
        print("sample {} grasps for {}: {}".format(len(grasp_list), world_obj, grasp_list))

        for grasp in grasp_list:
            print("find ik for grasp", grasp)

            ## find ik
            for ik in self.stream_map["inverse-kinematics-hand"](a=None, o=obj_body, p=obj_Pose, g=grasp[0]):
                if len(ik) == 0:
                    continue

                ## now we want to sample a trajectory from current position to next
                q2 = ik[0][0]
                for move_cmd in self.stream_map["plan-free-motion-hand"](q1=current_q, q2=q2):
                    print(move_cmd)
                    if len(move_cmd) == 0:
                        continue

                    ## computes actions to step the world
                    pick_action = ('pick_hand', ('hand', obj_body, obj_Pose, grasp[0], None, ik[0][1]))
                    move_action = ('move_cartesian', (current_q, ik[0][0], move_cmd[0][0]))
                    commands = []
                    for action in [move_action, pick_action]:
                        commands += get_primitive_actions(action, self.world)

                    ## update world state
                    self.current_g = grasp[0]

                    return commands

        return None

    def _get_place_action(self, object_name, surface_name):

        ## before we do anything
        if self.current_g is None:
            # there is nothing in hand to be placed
            return None

        # get current robot configuration
        current_q = Conf(self.robot, get_se3_joints(self.robot))

        surface = self.world.name_to_body(surface_name)

        obj_body = self.world.name_to_body(object_name)

        if surface_name == "cabinettop_storage":
            placement_stream = self.stream_map["sample-pose-in"]
        else:
            placement_stream = self.stream_map["sample-pose-on"]

        for placement_pose in placement_stream(obj_body, surface):

            print("find ik for placement pose", placement_pose)

            for ik in self.stream_map["inverse-kinematics-hand"](a=None, o=obj_body, p=placement_pose[0][0],
                                                                 g=self.current_g):
                print(ik)
                if len(ik) == 0:
                    continue

                ## motion plan to place
                q = ik[0][0]
                for move_cmd in self.stream_map["plan-free-motion-hand"](q1=current_q, q2=q):
                    print(move_cmd)
                    if len(move_cmd) == 0:
                        continue

                    ## computes actions to step the world
                    move_action = ('move_cartesian', (current_q, q, move_cmd[0][0]))
                    place_action = ('place_hand', ('hand', obj_body, placement_pose, self.current_g, None, ik[0][1]))
                    commands = []
                    for action in [move_action, place_action]:
                        commands += get_primitive_actions(action, self.world)

                    ## update world state
                    self.current_g = None

                    return commands

        return None

    #-------------------------------------------------------------------------------------------------------------------
    def initialize_symbolic_state(self):
        for obj in self.world.OBJECTS_BY_CATEGORY["moveable"]:
            self.symbolic_state[obj.name] = {"location": None, "cleanliness": None}
        self.symbolic_state["grasped"] = None

    # def update_symbolic_state(self, target_obj_name=None):
    def update_symbolic_state(self, target_obj_name=None, target_manip_name = None):

        ## first get some facts from world state
        facts = self.state.get_facts(init_facts=[], objects=None)
        relevant_predicates = ["supported", "contained", "cleaned"]
        predicates = {}                                                        
        for fact in facts:
            pred = fact[0].lower()
            if pred not in relevant_predicates:
                continue
            if pred not in predicates:
                predicates[pred] = []
            predicates[pred].append(fact)
        predicates = {k: v for k, v in sorted(predicates.items())}

        pred_to_list = {}
        for pred in predicates:
            list = [get_readable_list(fa, self.world) for fa in predicates[pred]]
            pred_to_list[pred] = list

        # parse these world facts to update the symbolic state
        # Debug: there is a bug that the object is still supported even though it is contained. therefore, the order
        #        of preds matter
        preds = ["supported", "contained"]
        for pred in preds:
            if pred not in pred_to_list:
                continue
            for fact in pred_to_list[pred]:
                obj_name = fact[1].split("|")[1]
                loc_name = fact[3].split("|")[1]
                if "::" in loc_name:
                    loc_name = loc_name.split("::")[1]

                self.symbolic_state[obj_name]["location"] = loc_name

        if "cleaned" in pred_to_list:
            for fact in pred_to_list["cleaned"]:
                obj_name = fact[1].split("|")[1]
                self.symbolic_state[obj_name]["cleanliness"] = True

        #cg: if manip mane is clean, directly update state
        if target_manip_name == 'clean':
            self.symbolic_state[target_obj_name]["cleanliness"] = True

        ## update gripper state
        if self.current_g is not None:
            assert target_obj_name is not None
            self.symbolic_state["grasped"] = target_obj_name
        else:
            self.symbolic_state["grasped"] = None

        print(f"\nCurrent symbolic state:\n{self.symbolic_state}\n")

    def parse_symbolic_goal(self):
        for fact in self.goal:
            if fact[0] == "In":
                # print(fact[0], fact[1].name, self.world.BODY_TO_OBJECT[fact[2]].name)
                obj_name = fact[1].name
                loc_name = self.world.BODY_TO_OBJECT[fact[2]].name
                if "::" in loc_name:
                    loc_name = loc_name.split("::")[1]
                self.symbolic_goal[obj_name] = loc_name
            if fact[0] == "On":
                # print(fact[0], fact[1].name, self.world.BODY_TO_OBJECT[fact[2]].name)
                obj_name = fact[1].name
                loc_name = self.world.BODY_TO_OBJECT[fact[2]].name
                if "::" in loc_name:
                    loc_name = loc_name.split("::")[1]
                self.symbolic_goal[obj_name] = loc_name

        print(f"\nGoal symbolic state:\n{self.symbolic_goal}\n")

    def check_symbolic_action_feasibility(self, manip_name, obj_name, loc_name):
        if manip_name == "pick":
            if self.symbolic_state["grasped"] != None:
                return False
            if loc_name != self.symbolic_state[obj_name]["location"]:
                return False

        if manip_name == "place":
            if self.symbolic_state["grasped"] == None:
                return False
            if obj_name != self.symbolic_state["grasped"]:
                return False
            if loc_name == "cabinettop_storage" and not self.symbolic_state[obj_name]["cleanliness"]:
                return False

        return True

    def check_reach_symbolic_goal(self):
        at_goal = True
        for obj_name in self.symbolic_goal:
            if self.symbolic_goal[obj_name] != self.symbolic_state[obj_name]["location"]:
                at_goal = False
                break

        return at_goal

#CG: make reset environment have random semantics
class RandomSemanticSpecWrapper(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)

    def reset(self):
        filenames = [filename for filename in os.listdir(config.semantic_specs_dir) if filename.endswith('.json')]
        semantic_spec = random.choice(filenames)
        semantic_spec_seed = os.path.splitext(semantic_spec)[0]
        seed = random.randint(0, 10)

        new_config = copy.deepcopy(config)
        new_config.seed = seed
        set_random_seed(seed)
        set_numpy_seed(seed)

        semantic_spec_file = os.path.join(config.semantic_specs_dir, semantic_spec)
        semantic_spec_dict = load_dict_from_json(semantic_spec_file)
        obj_dict = semantic_spec_dict["objects"]
        goal_dict =semantic_spec_dict["goals"]
        print(obj_dict)
        print(goal_dict)
        new_config.obj_dict = obj_dict
        new_config.goal_dict = goal_dict

        exp_dir = abspath(join(config.data.out_dir, "semantic_spec_{}_seed_{}_time_".format(semantic_spec_seed, seed))) #+ get_datetime(TO_LISDF=True)))
        print(exp_dir)
        os.makedirs(exp_dir, exist_ok=True)
        new_config.data.out_dir = exp_dir

        new_config.world.builder_kwargs["semantic_spec_file"] = os.path.join(config.semantic_specs_dir, semantic_spec)

        """ STEP 1 -- GENERATE SCENES """
        file = create_pybullet_world(new_config, SAVE_LISDF=False, SAVE_TESTCASE=True, RESET=True)
        world, goal = create_pybullet_world(new_config, SAVE_LISDF=False, SAVE_TESTCASE=True)

        self.env = CleanDishEnvV1(world, goal, new_config, render_mode="human")
        self.env.reset()
        observation = self.env.reset()

        return observation

def get_facts(world, state):
    facts = state.get_facts(init_facts=[], objects=None)
    # print_fn = parallel_print ## if args.parallel else myprint
    # print_fn(config)
    predicates = {}
    for fact in facts:
        pred = fact[0].lower()
        if pred not in predicates:
            predicates[pred] = []
        predicates[pred].append(fact)
    predicates = {k: v for k, v in sorted(predicates.items())}
    for pred in predicates:
        print(pred)
        list = [get_readable_list(fa, world) for fa in predicates[pred]]
        print(list)

    summarize_facts(facts, world, name='Facts', print_fn=print)

def collect_for_fastamp():

    if config.env_seed is not None and config.semantic_spec is not None:
        inputs = [{"semantic_spec": config.semantic_spec, "env_seed": config.env_seed}]
    else:
        semantic_specs = get_semantic_specs(config.semantic_specs_dir)
        inputs = []
        for spec in semantic_specs[23:]:
            for env_seed in range(10):
                inputs.append({"semantic_spec": spec, "env_seed": env_seed})

    parallel_processing(process, inputs, parallel=config.parallel)


if __name__ == '__main__':
    collect_for_fastamp()
